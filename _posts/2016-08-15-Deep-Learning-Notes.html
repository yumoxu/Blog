---
layout: default
title: Deep Learning Notes
--
<head>
    <link rel="stylesheet" type="text/css" href="note_style.css" />
    <script type="text/javascript" async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
</head>

<h1>{{ page.title }}</h1>

<h2>Problems in deep learning with BP</h2>
<li>Too many parameters => lack of labelled data => overfitting.</li>
<li>Non-convex optimization problem => local extrema. </li>
<li>Diffusion gradient.</li>

<h2>Solutions</h2>
<li>Auto-encoders: utilize unlabelled data</li>
<ul>
    <li>Forward Network </li>
    <li>RBM </li>
</ul>
<li>CNN: compress parameters</li>

<h2>Auto-encoders</h2> 
<p>Given inputs x = {x_i, x_2, ..., x_n}, find $f()$, $g()$ that satisfies $g(f(x)) \approx x$. $f()$ is the encoder and $g()$ is the decoder. Then, $X = f(x) = {X_1ï¼ŒX_2,...,X_m}$ can be regarded as features of the input $x$. </p>
<p>The main task here is to learn parameters in $f()$ and $g()$: </p>
<h3>1. Auto-encoders with FNN </h3>
<p>We first use 3-layer FNN to build an auto-encoder. To obtain better features, we can make $ m < n $, or make $X$ sparse.</p>
<p>The auto-encoder is called <i>sparse auto-encoders</i> in the latter situation, where we use K-L divergence to measure the sparsity in the loss function.</p>
<p>Here, corresponding to the 3-layer FNN structure, $f()$ is the non-linear function transferring $z$ to $a$, and $g()$ is a linear function: $g(x) = wx+b$. We can use BP algorithm here to learn the parameters because the FNN is relatively shallow.</p>
<p>We can also use the network with more layers to build the auto-encoder. There are mainly 2 steps: <b>pre-training</b> and <b>fine-training</b>. The learning method used in the pre-training is called <i>layerwise-learning</i>. To be more concret, we train the sub-network with the first 2 layers and a manually-added hidden layer as an auto-encoder, after which we have $f()$, $g()$ and the encoded value $X$ in the 2th layer. Then, we regard $X$ as the inputs, and train another shallow auto-encoder together with the 3rd layer and another manually-added hiden layer...Continue the loop until approaching the last layer. Thus, we have all the $f()$, $g()$ as well as initial values in each layer.</p>
<p>As to the fine-training, we regard all the encoding process in the layer-wise learning as a whole, so do the decoding process. We use $C$, $D$ to denote encoding and decoding process, repectively. The original process $C \to D \to C \to D \to ... \to C \to D$ can then be regarded as $C \to C \to ... \to C (\to X) \to D \to D \to ...\to D$. We then have a multi-layer auto-encoder. As to the multi-layer auto-encoder, thanks to the pre-training step, we have good initial values in each layer, thus BP algorithm can be utilized again to fine-train all the parameters. The final $X$ here is called <i>deep feature</i>. </p>
<p>Deep features can be applied to supervised classfication problems. What is noteworthy here is that if big data is available, we can fine-train again the parameters in the encoding function together with the classification function supervisely. </p>
<h3>2. Anto-encoders with RBM (Restricted Boltzmann Machine) and DBM (Deep Boltzmann Machine)</h3>
<p>Reference: <i>Reducing the dimensionality of Data by NN, Hinton</i></p>
<h4>2.1 RBM</h4>
<p>Definition: a simple probabilistic undirected graph. Input: $x = (v_1, v_2, ..., v_m, h_1, h_2, ..., h_n) ^ T$. $v_i, h_j \in {0,1}$ Model:
\[P(x) = P(v,h) = \frac{1}{Z}\exp\{\sum_i{a_i v_i} + \sum_j{b_j h_j} + \sum{w_{ij} v_i h_j}\}\] 
<i>Why called restricted</i>: no edges in inner $v$ and inner $h$. 
</p>
<div align = "center">
    <img src = '/images/2016-08-15-Deep-Learning-Notes/rbm.png' width="200">
</div>
<p>Learning: use <i>CD Algorithm</i> to calculate patial derivatives for each parameter in the MLE function, then gradient descend it.</p>
<h4>2.2 DBM</h4>
<p>Definition: extend RBM. $v_i \in [0,1], h_j \in {0,1}$.</p>
<p>Training: pre-training (layer-wise learning) and fine-training, similar to multi-layer auto-encoders with FMM.</p>
<div align = "center">
    <img src = '/images/2016-08-15-Deep-Learning-Notes/dbm.png' width="200">
</div>
<h2>CNN</h2>
<p>Widely applied in CV. Take advantage of the concept of <i>filter</i> in the signal processing field.</p>
<h3>1. Convolutional layer</h3>
<p>The filter is <b>size-fixed</b>, e.g. $m * n$ in 2-dimensional situation. Each node in convolutional layer $l$ represents certain information of several nodes in layer $l-1$ through a filter and a <i>connection table</i>. The filters connected to each node in layer $l$ are different, which are <b>parameters to be learnt</b>. The connection table defines the connection status between layer $l-1$ and layer $l$. The node value in layer $l$ equals to the sigmoid function taking the summation of correlated nodes in layer $l-1$ as the input.</p>
<h3>2. Sub-sampling and pooling layer</h3>
<p>We <i>sub-sample</i> 2-dimensional values $(p, q, r, s)$ in each node in layer $l-1$, then <i>pooled</i> them to $sigmoid(w * (p+q+r+s) + b)$, where $w$ and $b$ are parameters to be leant. The <i>field size</i> here is 2. $2 * 2$ is then compressed to $1 * 1$.</p> 
<p>Generally we connect a sub-sampling and pooling layer after a convolutional layer.</p>

<h2>NLP Applications</h2>
<h3>1. NNLM</h3>
<p>Model:</p>
<div align = 'center'>
    <img src = '/images/2016-08-15-Deep-Learning-Notes/nnlm.png' width="400">
</div>
<p>Explanation:
<li>Input size: $n-1$. Dictionary size: $|D|$</li>
<li>$w_{t-i} \to C(w_{t-i})$: Table look-up. Usually one-hot representation, so represenation size $m = |D|$.</li>
<li>tanh layer: Linear + nonlinear transformation. Ouput: $Z = tanh(Hx + b)$, where $H \in R^{(m * (n-1)) * h}$.</li>
<li>Softmax layer: Linear + nonlinear transformation. $Y = softmax(W * Z + d)$, where $W \in R^{|D| * h}$.</li>
<li>Final result: $p(w_t|context) = \frac{e^{y_t}}{\sum_{i}^{|D|} e^{y_i}}$.</li>
</p>

<h3>2. Word Embedding Learning</h3>
Given $w_t$ and the window size, for example, 2, we have $w_t$'s window $w_{t-2}^{t+2} = {w_{t-2}, w_{t-1}, w_{t}, w_{t+1}, w_{t+2}}$. If we replace $w_t$ with another word, a negative sample will be generated. Thus, we can transfer word embedding learning problem to supervised 2-class classification problem. We randomly initialize $C(w_i)$ in the first layer of NN, then linear transfer them, feed to SVM, then learn the word embeddings through BP.
<h3>3. Other NLP Tasks</h3>
<p>Tasks: POS, chunking, NER, SRL, etc.</p>
<p>The first 3 tasks can be regarded as tagging problem. SRL aims to find predicates (or verbs) in the sentence and their respective auguments, such as subjects and objects.</p>
<p>Word Embeddings learnt can be used to build the look-up table, transferring words to vectors. Continuing framwork is similar with word embedding learning.</p>

<p>{{ page.date | date_to_string }}</p>
