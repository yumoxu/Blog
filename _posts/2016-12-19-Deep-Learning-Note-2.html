---
layout: default
title: Deep Learning Note 2
---
<head>
    <link rel="stylesheet" type="text/css" href="note_style.css" />
    <script type="text/javascript" async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <!-- Build mathjax support-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
</head>

<h1>{{ page.title }}</h1>

<h2>RNN</h2>
<h3>Model:</h3>

<div align = "center">
    <img src = '/images/2016-12-19-Deep-Learning-Note-2/RNN.png' width = "300">
</div>

<div align = "center">
    <img src = "/images/2016-12-19-Deep-Learning-Note-2/The_inputs_and_outputs_to_a_neuron_of_a_RNN.png" width = "250">
</div>

<h3>Algorithm:</h3>
<h3>Problem: the vanishing/exploding gradient</h3>
<p>Forwad propagation:</p>
\[h_t = 
\begin{pmatrix} W^{(hh)} & W^{(hx)} \end{pmatrix} 
\begin{pmatrix} f({h_{t-1}}) \\ X_t \end{pmatrix}\]
\[\hat{y_t} = W^{(s)}f(h_t)\]
<p>Totol error:</p>
\[E = \sum_{t=1}^T E_t\]
<p>Now, backpropogate the most difficult parametre $W^{hh}$:</p>
\[
\frac{\partial E}{\partial W^{(hh)}} = 
\sum_{t=1}^{T} \frac{\partial E_t}{W^{(hh)}}
\]
<p>where according to the hard-core chain rule, we have</p>
\[
\frac{\partial E_t}{\partial W^{(hh)}}
= 
\sum_{k=1}^t
\frac{\partial E_t}{\partial \hat{y_t}}
\frac{\partial \hat{y_t}}{\partial h_t}
\frac{\partial h_t}{\partial h_k}
\frac{\partial h_k}{\partial W^{(hh)}}
\]
<p>Now, the most dynamic part we sum over:</p>
\[
\frac{\partial h_t}{\partial h_k} =
\prod_{j = k+1}^t \frac{\partial h_j}{\partial h_{j-1}}
\]
<p>We know that the numerator and the denominator of the element we product over are both vectors, so we have the Jacobian Matrix:</p>
\[
\frac{\partial h_j}{\partial h_{j-1}} = 
\begin{bmatrix}
\frac{\partial h_j}{\partial h_{j-1, 1}} &
\cdots &
\frac{\partial h_j}{\partial h_{j-1, n}}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial h_{j,1}}{\partial h_{j-1, 1}} & \cdots & \frac{\partial h_{j,n}}{\partial h_{j-1, 1}} 
\vdots & \ddots & \vdots 
\frac{\partial h_{j,n}}{\partial h_{j-1, 1}} & \cdots & \frac{\partial h_{j,n}}{\partial h_{j-1, n}}
\end{bmatrix}
\]
<p>Here, we need to compute $\frac{\partial h_{j,p}}{\partial h_{j-1, q}}$. The final result is (check by yourself):</p>
\[
\frac{\partial h_{j,p}}{\partial h_{j-1, q}}=
W^{T} \mathrm{diag}[f^\prime(h_{j-1})]
\]
<h2>Solutions</h2>
<ul>
    <li>GRU: Gated Unit</li>
    <li>LSTM: Long-short-term memory</li>
</ul>

<h2>GRU</h2> 
<p>
Definition:

</p>

<div align = "center">
    <img src = '/images/2016-12-19-Deep-Learning-Note-2/GRU.png' width="500">
</div>

<h2>LSTM</h2>
<p>Definition:</p>

<!--Three LSTM Figures-->
<div align = "center">
    <img src = '/images/2016-12-19-Deep-Learning-Note-2/LSTM1.png' width="500">
</div>

<div align = "center">
    <img src = '/images/2016-12-19-Deep-Learning-Note-2/LSTM2.png' width="500">
</div>

<div align = "center">
    <img src = '/images/2016-12-19-Deep-Learning-Note-2/LSTM3.jpg' width="500">
</div>
<p>{{ page.date | date_to_string }}</p>
